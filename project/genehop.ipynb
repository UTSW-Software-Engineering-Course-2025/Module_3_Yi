{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a8b93f",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "# Starter Notebook for GeneTuring Few-shot Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7260d323",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Let's start of by implementing a basic harness to evaluate the Geneturing dataset using Ollama with a Few-Shot prompting strategy to evaluate the performance of these models before implementing the tool-using strategies outlined in the GeneGPT paper.\n",
    "\n",
    "As with most benchmarking code using pretrained models, our notebook will following typical outline of:\n",
    "\n",
    "1. **Imports**\n",
    "2. **Configuration**\n",
    "3. **Data Loading**\n",
    "4. **Model Specification**\n",
    "5. **Metrics**\n",
    "6. **Evaluation Loop**\n",
    "7. **Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ecb4f5",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db83b1e",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Good hygiene for Jupyter notebooks includes placing all of the imports at the top of the notebook. This makes it easier to understand what dependencies are needed to run the notebook for new users and mirrors good practices for Python scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb9107c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Place imports here\n",
    "from typing import List, Optional\n",
    "\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c15925",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cf9042",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Also, let's create a section that is specific to the configuration of the run. This will make it easier to change the configuration of the run without hunting for hard-coded values sprinkled throughout the code and makes it easier for others to understand the configuration of the run.\n",
    "\n",
    "We will leave it empty at the moment, but we will come back and fill it in as we identify global configuration options that we need to implement the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71d543d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 Data Configuration\n",
    "@dataclass\n",
    "class DataConfig:\n",
    "    input_path: str = \"data/genehop.json\"     # 数据输入文件\n",
    "    output_dir: str = \"results/\"                 # 结果输出目录\n",
    "    dataset_format: str = \"json\"                 # 数据格式\n",
    "    task_field: str = \"question\"                 # 输入字段名\n",
    "    answer_field: str = \"answer\"                 # 标准答案字段名\n",
    "    split_key: str | None = None                 # 可选：用于分任务或切分数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e42308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 Model Configuration\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    model_name: str = \"gpt-4.1\"                     # \"gpt-4\", \"gpt-3.5-turbo\"\n",
    "    max_tokens: int = 512\n",
    "    temperature: float = 0.7\n",
    "    top_p: float = 0.9\n",
    "    top_k: Optional[int] = None                   \n",
    "    use_cuda: bool = False                       \n",
    "    batch_size: int = 1                        \n",
    "    model_backend: str = \"openai\"                 \n",
    "    model_variant: Optional[str] = None          \n",
    "    openai_api_key: Optional[str] = None          \n",
    "    openai_base_url: Optional[str] = \"https://api.openai.com/v1\"  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac0d904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3 Evaluation and Logging Configuration\n",
    "@dataclass\n",
    "class EvaluationConfig:\n",
    "    evaluate_per_task: bool = True              # 是否按任务类型评估（如问答 vs 生成）\n",
    "    save_failed_cases: bool = True              # 是否保存失败样本\n",
    "    failed_cases_path: str = \"results/failed.json\"  # 保存失败样本的位置\n",
    "    metrics: tuple[str, ...] = (\"accuracy\",)    # 使用哪些指标，如 accuracy, f1\n",
    "\n",
    "@dataclass\n",
    "class LoggingConfig:\n",
    "    enable_mlflow: bool = True                  # 是否启用 MLflow\n",
    "    experiment_name: str = \"GeneTuring-Run\"     # MLflow 实验名\n",
    "    run_name: Optional[str] = None              # 可选的具体 run 名称\n",
    "    tracking_uri: Optional[str] = None          # 若设置为 None 则使用默认本地 URI\n",
    "    log_artifacts: bool = True                  # 是否记录生成文件"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1904443",
   "metadata": {},
   "source": [
    "## 3. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d7a1f",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "In this section we need to load the data that we will be using for our evaluation. \n",
    "\n",
    "The JSON file at `data/gene_turing.json` contains a dictionary of dictionaries. At the top level, the keys are the names of the tasks. Within each task, there are several key-value pairs where the keys are the questions and the values are the answers.\n",
    "\n",
    "Below is one example for each of the 9 tasks in the dataset:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"Gene alias\": {\n",
    "        \"What is the official gene symbol of LMP10?\": \"PSMB10\"\n",
    "    },\n",
    "    \"Gene disease association\": {\n",
    "        \"What are genes related to Hemolytic anemia due to phosphofructokinase deficiency?\": \"PFKL\"\n",
    "    },\n",
    "    \"Gene location\": {\n",
    "        \"Which chromosome is FAM66D gene located on human genome?\": \"chr8\"\n",
    "    },\n",
    "    \"Human genome DNA aligment\": {\n",
    "        \"Align the DNA sequence to the human genome:ATTCTGCCTTTAGTAATTTGATGACAGAGACTTCTTGGGAACCACAGCCAGGGAGCCACCCTTTACTCCACCAACAGGTGGCTTATATCCAATCTGAGAAAGAAAGAAAAAAAAAAAAGTATTTCTCT\": \"chr15:91950805-91950932\",\n",
    "    },\n",
    "    \"Multi-species DNA aligment\": {\n",
    "        \"Which organism does the DNA sequence come from:AGGGGCAGCAAACACCGGGACACACCCATTCGTGCACTAATCAGAAACTTTTTTTTCTCAAATAATTCAAACAATCAAAATTGGTTTTTTCGAGCAAGGTGGGAAATTTTTCGAT\": \"worm\",\n",
    "    },\n",
    "    \"Gene name conversion\": {\n",
    "        \"Convert ENSG00000215251 to official gene symbol.\": \"FASTKD5\",\n",
    "    },\n",
    "    \"Protein-coding genes\": {\n",
    "        \"Is ATP5F1EP2 a protein-coding gene?\": \"NA\",\n",
    "    },\n",
    "    \"Gene SNP association\": {\n",
    "        \"Which gene is SNP rs1217074595 associated with?\": \"LINC01270\",\n",
    "    },\n",
    "    \"SNP location\": {\n",
    "        \"Which chromosome does SNP rs1430464868 locate on human genome?\": \"chr13\",\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61e217f",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "\n",
    "We need to reformat this into a pandas dataframe with the following columns:\n",
    "- `id`: A serial ID number we will assign to each example (int)\n",
    "- `task`: The name of the task (str)\n",
    "- `question`: The question for the example (str)\n",
    "- `answer`: The answer for the example (str)\n",
    "\n",
    "The final dataframe we will create should look like this:\n",
    "\n",
    "| id | task | question | answer |\n",
    "|----|------|----------|--------|\n",
    "| 0 | Task1 | Question1 | Answer1 |\n",
    "| 1 | Task1 | Question2 | Answer2 |\n",
    "| 2 | Task2 | Question1 | Answer1 |\n",
    "| 3 | Task2 | Question2 | Answer2 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c2b9c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 150 examples across 3 tasks.\n",
      "Tasks: ['Disease gene location', 'SNP gene function', 'sequence gene alias']\n"
     ]
    }
   ],
   "source": [
    "# 3.1 Load the JSON file\n",
    "\n",
    "# Load the data here\n",
    "\n",
    "# Build the TASKS variable here\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import Dict, Any, List\n",
    "import os\n",
    "os.chdir(\"/project/bioinformatics/WZhang_lab/s440820/Module_3_Yi/project\")\n",
    "\n",
    "def load_and_flatten_gene_turing_data(config: DataConfig) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the nested GeneTuring QA dataset and flattens it into a dataframe.\n",
    "\n",
    "    Args:\n",
    "        config (DataConfig): Configuration containing the input file path.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe with columns ['task', 'question', 'answer']\n",
    "    \"\"\"\n",
    "    # Step 1: Load raw nested JSON\n",
    "    with open(config.input_path, \"r\") as f:\n",
    "        raw_data: Dict[str, Dict[str, str]] = json.load(f)\n",
    "\n",
    "    # Step 2: Flatten into list of dicts\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    for task_name, qa_pairs in raw_data.items():\n",
    "        for question, answer in qa_pairs.items():\n",
    "            rows.append({\n",
    "                \"task\": task_name,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer\n",
    "            })\n",
    "\n",
    "    # Step 3: Convert to DataFrame\n",
    "    df = pd.DataFrame(rows)[[\"task\", \"question\", \"answer\"]]\n",
    "\n",
    "    return df\n",
    "data_config = DataConfig(input_path=\"data/genehop.json\")\n",
    "df = load_and_flatten_gene_turing_data(data_config)\n",
    "TASKS = set(df[\"task\"])\n",
    "\n",
    "print(f\"Loaded {len(df)} examples across {len(TASKS)} tasks.\")\n",
    "print(\"Tasks:\", sorted(TASKS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe9a5062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.2 Iterate through the JSON data recursively to collect each of the rows into a list\n",
    "#     Each row should have a dictionary with keys of the columsn in the table above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bdff6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3 Create the pandas dataframe from the collection of rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f330e0",
   "metadata": {},
   "source": [
    "## 4. Model Specification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0dff55",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "With our few-shot in-context learning model, we need to specify four components:\n",
    "\n",
    "1. The large language model to use\n",
    "2. The instructions for the model as a system prompt\n",
    "3. The few-shot examples to provide to the model to demonstrate the input-output format\n",
    "4. The completion request function that puts it all together retrieving a response for each unseen input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a9f18",
   "metadata": {},
   "source": [
    "### 4.1 Setting up the large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f0a94",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "We will use the Ollama client to interface with the large language model on the Ollama server we started. With large language models, it is common to use a client library to interface with the model hosted by a server. This allows us to iterate quickly on the prompting and post-processing logic without having to incur the overhead of loading the model into memory each time.  Additionally, model code is oftentimes optimized for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e045007a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Setting up the large language model Ollama model client\n",
    "import requests\n",
    "from typing import Optional\n",
    "from openai import OpenAI\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "def query_model(prompt: str, config: ModelConfig) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Query a large language model using the specified backend (ollama or openai).\n",
    "    \"\"\"\n",
    "    if config.model_backend == \"ollama\":\n",
    "        # Using Ollama's local HTTP API\n",
    "        url = \"http://localhost:11434/api/generate\"\n",
    "        payload = {\n",
    "            \"model\": config.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": config.temperature,\n",
    "                \"top_p\": config.top_p,\n",
    "                \"top_k\": config.top_k,\n",
    "                \"num_predict\": config.max_tokens\n",
    "            }\n",
    "        }\n",
    "        try:\n",
    "            response = requests.post(url, json=payload)\n",
    "            response.raise_for_status()\n",
    "            return response.json().get(\"response\", \"\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"[Ollama Error] {e}\")\n",
    "            return None\n",
    "\n",
    "    elif config.model_backend == \"openai\":\n",
    "        # Using OpenAI-compatible API (e.g., local LLM with OpenAI wrapper)\n",
    "        client = OpenAI(\n",
    "        api_key=config.openai_api_key\n",
    "        )\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=config.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=config.temperature,\n",
    "                top_p=config.top_p,\n",
    "                max_tokens=config.max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"[OpenAI API Error] {e}\")\n",
    "            return None\n",
    "    elif config.model_backend == \"azure\":\n",
    "        client = AzureOpenAI(\n",
    "            api_key=config.openai_api_key,\n",
    "            azure_endpoint=config.openai_base_url,\n",
    "            api_version=\"2024-03-01-preview\"\n",
    "        )\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=config.model_name,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=config.temperature,\n",
    "                top_p=config.top_p,\n",
    "                max_tokens=config.max_tokens\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        except Exception as e:\n",
    "            print(f\"[Azure API Error] {e}\")\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backend: {config.model_backend}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "607889e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm called ChatGPT! I am an AI language model developed by OpenAI, a research organization that focuses on artificial intelligence. If you have any questions or need help, feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "model_config = ModelConfig(\n",
    "    model_name=\"gpt-4.1\",\n",
    "    model_backend=\"azure\",\n",
    "    openai_api_key=\"FJ5GrEV5LG3Y0UIeac29BIhmVu8GPcmWyeTTFH0cBifgT7T68XHPJQQJ99BEACHYHv6XJ3w3AAAAACOGdwHb\",\n",
    "    openai_base_url=\"https://michaelholcomb-5866-resource.cognitiveservices.azure.com/\"\n",
    ")\n",
    "\n",
    "output = query_model(\"What your name? who develop this model?\", config=model_config)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8caf45c9",
   "metadata": {},
   "source": [
    "### 4.2 Setting up the system prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46af38",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Modern large language models are post-trained to perform a variety of tasks and follow instructions. To leverage this capability, we need to provide a system prompt that clearly outlines the task, any constraints, and the format of the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29bf3d5",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Designing the system prompt is a critical aspect of using LLMs. Below are several resources for designing a system prompt:\n",
    "* [OpenAI Prompt Engineering](https://platform.openai.com/docs/guides/text?api-mode=responses#prompt-engineering)\n",
    "* [Kaggle/Google Prompt Engineering](https://www.kaggle.com/whitepaper-prompt-engineering?_bhlid=a2bfce2cac67662098bd85a241e7cb000576e5d4)\n",
    "* [Anthropic Prompt Engineering](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview)\n",
    "* [OpenAI GPT 4.1 Prompting Cookbook](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887a248",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "**From the OpenAI Prompt Engineering guide:**\n",
    "\n",
    "> **Identity**: Describe the purpose, communication style, and high-level goals of the assistant. \n",
    "> \n",
    "> **Instructions**: Provide guidance to the model on how to generate the response you want. What rules should it follow? What should the model do, and what should the model never do? This section could contain many subsections as relevant for your use case, like how the model should call custom functions.  \n",
    ">\n",
    "> **Examples**: Provide examples of possible inputs, along with the desired output from the model.  \n",
    "> \n",
    "> **Context**: Give the model any additional information it might need to generate a response, like private/proprietary data outside its training data, or any other data you know will be particularly relevant. This content is usually best positioned near the end of your prompt, as you may include different context for different generation requests.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcebc861",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "This is what the system prompt looked like in the originalGeneGPT paper, but it is not the best. Identify what it includes and what is missing. Implement your own system prompt incorporating best practices from some of the guides posted above. \n",
    "\n",
    "> \t'Hello. Your task is to use NCBI Web APIs to answer genomic questions.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d73bcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.2 Draft your own system prompt for our generic genomics question answering system. \n",
    "#     Replace the system message `content` below with your own.\n",
    "system_content = \"\"\"[\n",
    "You are GeneGPT, an expert genomic assistant designed to answer a wide range of questions about genes, SNPs, chromosomal locations, and related biological knowledge using reliable sources such as NCBI databases.\n",
    "'You can call Eutils by: \"[https://eutils.ncbi.nlm.nih.gov/entrez/eutils/{esearch|efetch|esummary}.fcgi?db={gene|snp|omim}&retmax={}&{term|id}={term|id}]\".\\n' \\\n",
    "    'esearch: input is a search term and output is database id(s).\\n' \\\n",
    "    'efectch/esummary: input is database id(s) and output is full records or summaries that contain name, chromosome location, and other information.\\n' \\\n",
    "    'Normally, you need to first call esearch to get the database id(s) of the search term, and then call efectch/esummary to get the information with the database id(s).\\n' \\\n",
    "    'Database: gene is for genes, snp is for SNPs, and omim is for genetic diseases.\\n\\n' \\\n",
    "    'For DNA sequences, you can use BLAST by: \"[https://blast.ncbi.nlm.nih.gov/blast/Blast.cgi?CMD={Put|Get}&PROGRAM=blastn&MEGABLAST=on&DATABASE=nt&FORMAT_TYPE={XML|Text}&QUERY={sequence}&HITLIST_SIZE={max_hit_size}]\".\\n' \\\n",
    "    'BLAST maps a specific DNA {sequence} to its chromosome location among different specices.\\n' \\\n",
    "## Instructions:\n",
    "- Always answer using short and precise scientific terminology.\n",
    "- If the answer is not available or uncertain, reply with \"NA\".\n",
    "- Do not provide explanations unless explicitly asked.\n",
    "- Only return the exact answer string, without additional commentary or punctuation.\n",
    "- Respect formatting constraints provided in examples (e.g., \"chr13\" for chromosomes, official gene symbols like \"PSMB10\").\n",
    "- you can also use web search to get the answer if you need.\n",
    "\n",
    "\n",
    "## Context:\n",
    "You are responding as part of an automated system for benchmarking LLM performance on structured genomic queries.\n",
    "]\"\"\"\n",
    "\n",
    "system_message = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system_content\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb0bf3f",
   "metadata": {},
   "source": [
    "### 4.3 Setting up the few-shot examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8d700",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "For tasks with idiosyncratic output formats or constraints, it is important to provide clear instructions as well as examples to guide the model in generating the desired output. Mechanically, we provide these pairs of inputs and outputs as a sequence of user and assistant messages after the system prompt.\n",
    "\n",
    "```python\n",
    "messages += [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":  <fill in input example 1>\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": <fill in output example 1>\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": <fill in input example 2>\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": <fill in output example 2>\n",
    "    }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13fc96d8",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "In the GeneGPT code, the authors included several tasks, one of each of a subset of the tasks in the dataset. We will use the same examples here.\n",
    "\n",
    "Please inspect the GeneGPT repository to find the few-shot examples in the prompt. \n",
    "\n",
    "Specifically the `get_prompt_header` function in `main.py` located here: [main.py](https://github.com/ncbi/GeneGPT/blob/main/main.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcb78ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.3 Appending the few-shot examples to the `messages` list\n",
    "import time\n",
    "import urllib\n",
    "import re\n",
    "def call(url):\n",
    "    time.sleep(1)\n",
    "    url = url.replace(' ', '+')\n",
    "    req = urllib.request.Request(url)\n",
    "    with urllib.request.urlopen(req) as resp:\n",
    "        call = resp.read()\n",
    "    return call\n",
    "\n",
    "def format_call(url):\n",
    "    return f'[{url}]->[{call(url)}]\\n'\n",
    "\n",
    "a6_url = 'https://blast.ncbi.nlm.nih.gov/blast/Blast.cgi?CMD=Put&PROGRAM=blastn&MEGABLAST=on&DATABASE=nt&FORMAT_TYPE=XML&QUERY=ATTCTGCCTTTAGTAATTTGATGACAGAGACTTCTTGGGAACCACAGCCAGGGAGCCACCCTTTACTCCACCAACAGGTGGCTTATATCCAATCTGAGAAAGAAAGAAAAAAAAAAAAGTATTTCTCT&HITLIST_SIZE=5'\n",
    "a6 = call(a6_url)\n",
    "a6 = re.search('RID = (.*)\\n', a6.decode('utf-8')).group(1)\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": 'Here are a few examples. Make sure to give results in this format only:\\n'\n",
    "                    + 'Question: Which gene is SNP rs1217074595 associated with?\\n' \n",
    "                    + format_call('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=gene&retmax=5&retmode=json&sort=relevance&term=LMP10')\n",
    "                    + format_call('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=gene&retmax=5&retmode=json&id=19171,5699,8138')\n",
    "                    + 'Answer: PSMB10\\n\\n'\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": 'Question: Which gene is SNP rs1217074595 associated with?\\n'\n",
    "                    + format_call('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=snp&retmax=10&retmode=json&id=1217074595')\n",
    "                    + 'Answer: LINC01270\\n\\n'\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": 'Question: What are genes related to Meesmann corneal dystrophy?\\n'\n",
    "                    + format_call('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=omim&retmax=20&retmode=json&sort=relevance&term=Meesmann+corneal+dystrophy')\n",
    "                    + format_call('https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi?db=omim&retmax=20&retmode=json&id=618767,601687,300778,148043,122100')\n",
    "                    + 'Answer: KRT12, KRT3\\n\\n'\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": 'Question: Align the DNA sequence to the human genome:ATTCTGCCTTTAGTAATTTGATGACAGAGACTTCTTGGGAACCACAGCCAGGGAGCCACCCTTTACTCCACCAACAGGTGGCTTATATCCAATCTGAGAAAGAAAGAAAAAAAAAAAAGTATTTCTCT\\n'\n",
    "                    + f'[{a6_url}]->[{a6}]\\n'\n",
    "                    + format_call(f'https://blast.ncbi.nlm.nih.gov/blast/Blast.cgi?CMD=Get&FORMAT_TYPE=Text&RID={a6}')\n",
    "                    + 'Answer: chr15:91950805-91950932\\n\\n'\n",
    "    },\n",
    "]\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the official gene symbol of LMP10?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"PSMB10\\n\\n.\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Which chromosome is FAM66D gene located on human genome?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"chr8\\n\\n.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "example_messages += [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"Which chromosome does SNP rs1430464868 locate on human genome?\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"chr13\\n\\n.\"\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d89761",
   "metadata": {},
   "source": [
    "### 4.4 Implementing the model request function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4590511a",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Now we need to put it all together. We need a function that accepts as arguments:\n",
    "1. The client\n",
    "2. The system message\n",
    "3. The few-shot examples\n",
    "4. The new user query -- this case the user's question from the GeneTuring dataset.\n",
    "\n",
    "The function should return the response from the model and extract the answer (everything after 'Answer :' based on the format of the examples above)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8216c04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_client.py\n",
    "from typing import List, Dict, Optional, Type, Any\n",
    "from openai import AzureOpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "def query_gene_gpt_model(\n",
    "    client: AzureOpenAI,\n",
    "    system_prompt: str,\n",
    "    example_messages: List[Dict[str, str]],\n",
    "    user_query: str,\n",
    "    config: ModelConfig,\n",
    "    *,\n",
    "    pydantic_model: Optional[Type[BaseModel]] = None,   # ① Pydantic 模式\n",
    "    json_schema: Optional[Dict[str, Any]] = None        # ② 原始 schema 模式\n",
    ") -> str | dict:\n",
    "    \"\"\"\n",
    "    调用 Azure OpenAI (GPT-4o / GPT-4)：\n",
    "    - 如果传入 pydantic_model / json_schema，则返回结构化 JSON/Pydantic 对象\n",
    "    - 否则返回纯文本，并截取 'Answer:' 之后内容\n",
    "    \"\"\"\n",
    "    # 1. 组装消息\n",
    "    messages = (\n",
    "        [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "        + example_messages\n",
    "        + [{\"role\": \"user\", \"content\": user_query}]\n",
    "    )\n",
    "\n",
    "    # 2. 公共参数\n",
    "    base_kwargs = dict(\n",
    "        model=config.model_name,   # Azure 部署名\n",
    "        messages=messages,\n",
    "        temperature=config.temperature,\n",
    "        top_p=config.top_p,\n",
    "        max_tokens=config.max_tokens,\n",
    "    )\n",
    "\n",
    "    # ------------ 结构化输出路径 ------------\n",
    "    if pydantic_model or json_schema:\n",
    "        # a. 使用 pydantic（推荐，类型安全）\n",
    "        if pydantic_model is not None:\n",
    "            completion = client.beta.chat.completions.parse(\n",
    "                **base_kwargs, response_format=pydantic_model\n",
    "            )\n",
    "            return completion.choices[0].message.parsed  # 已是 pydantic 对象\n",
    "\n",
    "        # b. 仅要求 JSON（Azure 端不校验 schema，只保证合法 JSON）\n",
    "        completion = client.chat.completions.create(\n",
    "            **base_kwargs, response_format={\"type\": \"json_object\"}\n",
    "        )\n",
    "        return json.loads(completion.choices[0].message.content)\n",
    "\n",
    "    # ------------ 普通文本路径 ------------\n",
    "    completion = client.chat.completions.create(**base_kwargs)\n",
    "    content = completion.choices[0].message.content.strip()\n",
    "    return content.split(\"Answer:\")[-1].strip() if \"Answer:\" in content else content\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5dea011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: SLC38A6\n",
      "structure: answer='SLC38A6' confidence=0.99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# answer = query_gene_gpt_model(\n",
    "#     client=AzureOpenAI,\n",
    "#     system_prompt=\"You are a helpful assistant answering genomic questions using NCBI APIs.\",\n",
    "#     example_messages=example_messages,  \n",
    "#     user_query=\"Question: What is the official gene symbol of SNAT6?\",\n",
    "#     config=model_config\n",
    "# )\n",
    "model_config = ModelConfig(\n",
    "    model_name=\"gpt-4.1\",\n",
    "    model_backend=\"azure\",\n",
    "    openai_api_key=\"FJ5GrEV5LG3Y0UIeac29BIhmVu8GPcmWyeTTFH0cBifgT7T68XHPJQQJ99BEACHYHv6XJ3w3AAAAACOGdwHb\",\n",
    "    openai_base_url=\"https://michaelholcomb-5866-resource.cognitiveservices.azure.com/\"\n",
    ")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=model_config.openai_api_key,\n",
    "    azure_endpoint=model_config.openai_base_url,\n",
    "    api_version=\"2024-08-01-preview\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "answer_text = query_gene_gpt_model(\n",
    "    client=client,\n",
    "    system_prompt=system_content,\n",
    "    example_messages=example_messages,\n",
    "    user_query=\"What is the official gene symbol of SNAT6?\",\n",
    "    config=model_config,\n",
    ")\n",
    "print(\"text:\", answer_text)   # -> SLC38A6\n",
    "\n",
    "# 2. Pydantic 结构化模式\n",
    "class GeneHopResult(BaseModel):\n",
    "    answer: str\n",
    "    confidence: float\n",
    "\n",
    "answer_json = query_gene_gpt_model(\n",
    "    client=client,\n",
    "    system_prompt=system_content+\"\\nReturn result in JSON.\",\n",
    "    example_messages=example_messages,\n",
    "    user_query=\"What is the official gene symbol of SNAT6?\",\n",
    "    config=model_config,\n",
    "    pydantic_model=GeneHopResult,\n",
    ")\n",
    "print(\"structure:\", answer_json)    # -> GeneHopResult(answer='SLC38A6', confidence=0.93)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3103ef75",
   "metadata": {},
   "source": [
    "## 5. Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc440b3",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "* **Default exact match** - The predicted answers and ground truth are both strings. The score is 1 if they are equal and 0 otherwise\n",
    "* **Gene disease association** - The predicted answers and ground truth are both lists of gene-disease associations. The score is the proportion of correct associations present in the prediction\n",
    "* **Disease gene location** - The predicted and true answers are lists (e.g., gene locations related to a disease), and the evaluation calculates the fraction of the correct items present in the prediction.\n",
    "* **Human genome DNA aligment** - 1 point for exact match, 0.5 point if chrX part matches, 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "524dea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(answer, task):\n",
    "\n",
    "\tmapper = {'Caenorhabditis elegans': 'worm',\n",
    "\t\t\t  'Homo sapiens': 'human',\n",
    "\t\t\t  'Danio rerio': 'zebrafish',\n",
    "\t\t\t  'Mus musculus': 'mouse',\n",
    "\t\t\t  'Saccharomyces cerevisiae': 'yeast',\n",
    "\t\t\t  'Rattus norvegicus': 'rat',\n",
    "\t\t\t  'Gallus gallus': 'chicken'}\n",
    "\n",
    "\tif task == 'SNP location':\n",
    "\t\tanswer = answer.strip().split()[-1]\n",
    "\t\tif 'chr' not in answer:\n",
    "\t\t\tanswer = 'chr' + answer\n",
    "\n",
    "\telif task == 'Gene location':\n",
    "\t\tanswer = answer.strip().split()[-1]\n",
    "\t\tif 'chr' not in answer:\n",
    "\t\t\tanswer = 'chr' + answer\n",
    "\n",
    "\telif task == 'Gene disease association':\n",
    "\t\tanswer = answer.strip().replace('Answer: ', '')\n",
    "\t\tanswer = answer.split(', ')\n",
    "\n",
    "\telif task == 'Disease gene location':\n",
    "\t\tanswer = answer.strip().replace('Answer: ', '')\n",
    "\t\tanswer = answer.split(', ')\n",
    "\n",
    "\telif task == 'Protein-coding genes':\n",
    "\t\tanswer = answer.strip().replace('Answer: ', '')\n",
    "\t\tif answer == 'Yes':\n",
    "\t\t\tanswer = 'TRUE'\n",
    "\t\telif answer == 'No':\n",
    "\t\t\tanswer = 'NA'\n",
    "\n",
    "\telif task == 'Multi-species DNA aligment':\n",
    "\t\tanswer = answer.strip().replace('Answer: ', '')\n",
    "\t\tanswer = mapper.get(answer, answer)\n",
    "\n",
    "\telse:\n",
    "\t\tanswer = answer.strip().replace('Answer: ', '')\n",
    "\t\n",
    "\treturn answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c575ab56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.1 Implement metrics\n",
    "# 5.1 Implement metrics\n",
    "from collections import defaultdict\n",
    "from typing import List, Set, Union, Dict, Callable\n",
    "import re\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize text by converting to lowercase and removing extra whitespace.\"\"\"\n",
    "    return text.strip().lower()\n",
    "\n",
    "def _to_norm_set(items: Union[List[str], str]) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Convert input to a normalized set of strings.\n",
    "    \n",
    "    Args:\n",
    "        items: Either a list of strings or a comma/semicolon-separated string\n",
    "        \n",
    "    Returns:\n",
    "        Set of normalized strings\n",
    "    \"\"\"\n",
    "    if isinstance(items, list):\n",
    "        tokens = items\n",
    "    else:\n",
    "        # Split on both comma and semicolon, handle multiple delimiters\n",
    "        tokens = re.split(r'[,;]+', items)\n",
    "    return {normalize_text(tok) for tok in tokens if tok.strip()}\n",
    "\n",
    "def exact_match(pred: str, true: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate exact match score between prediction and ground truth.\n",
    "    \n",
    "    Args:\n",
    "        pred: Predicted answer\n",
    "        true: Ground truth answer\n",
    "        \n",
    "    Returns:\n",
    "        Float score: 1.0 for exact match, 0.0 otherwise\n",
    "    \"\"\"\n",
    "    return float(normalize_text(pred) == normalize_text(true))\n",
    "\n",
    "def gene_disease_association(pred: Union[List[str], str], true: Union[List[str], str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate gene-disease association score.\n",
    "    \n",
    "    Args:\n",
    "        pred: Predicted gene-disease associations\n",
    "        true: Ground truth gene-disease associations\n",
    "        \n",
    "    Returns:\n",
    "        Float score: Proportion of correct associations\n",
    "    \"\"\"\n",
    "    true_set = _to_norm_set(true)\n",
    "    pred_set = _to_norm_set(pred)\n",
    "    \n",
    "    # Handle empty ground truth case\n",
    "    if not true_set:\n",
    "        return 1.0 if not pred_set else 0.0\n",
    "    \n",
    "    # Calculate intersection over ground truth\n",
    "    correct = len(true_set & pred_set)\n",
    "    return correct / len(true_set)\n",
    "\n",
    "def disease_gene_location(pred: Union[List[str], str], true: Union[List[str], str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculate disease gene location score.\n",
    "    This is identical to gene-disease association scoring.\n",
    "    \"\"\"\n",
    "    return gene_disease_association(pred, true)\n",
    "\n",
    "def human_genome_dna_alignment(pred: str, true: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate human genome DNA alignment score.\n",
    "    \n",
    "    Scoring rules:\n",
    "    - 1.0 for exact match\n",
    "    - 0.5 if chromosome part matches\n",
    "    - 0.0 otherwise\n",
    "    \n",
    "    Args:\n",
    "        pred: Predicted alignment\n",
    "        true: Ground truth alignment\n",
    "        \n",
    "    Returns:\n",
    "        Float score: 1.0, 0.5, or 0.0\n",
    "    \"\"\"\n",
    "    pred_norm = normalize_text(pred)\n",
    "    true_norm = normalize_text(true)\n",
    "    \n",
    "    # Exact match\n",
    "    if pred_norm == true_norm:\n",
    "        return 1.0\n",
    "    \n",
    "    # Try to extract chromosome parts\n",
    "    try:\n",
    "        pred_chr = pred_norm.split(\":\")[0]\n",
    "        true_chr = true_norm.split(\":\")[0]\n",
    "        return 0.5 if pred_chr == true_chr else 0.0\n",
    "    except IndexError:\n",
    "        # If format is invalid, return 0\n",
    "        return 0.0\n",
    "\n",
    "# Map task names to their corresponding metric functions\n",
    "metric_task_map = defaultdict(\n",
    "    lambda: exact_match,                         # default_factory\n",
    "    {\n",
    "        \"gene disease association\": gene_disease_association,\n",
    "        \"disease gene location\": disease_gene_location,\n",
    "        \"human genome dna alignment\": human_genome_dna_alignment,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1ebe36",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Many of the gold-answers are in a specific format. `evaluate.py` also implements an answer post-processing function `get_answer` to better align model outputs with the gold answers. We also need to implement a similar function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bd36ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5.2 Implement the answer mapping function\n",
    "def score_answer(pred: str, true: str, task: str) -> float:\n",
    "    metric = metric_task_map[task.strip().lower()]\n",
    "    return metric(pred, true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413e73a3",
   "metadata": {},
   "source": [
    "## 6. Evaluation Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131aba90",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Now, let's implement the evaluation loop for the GeneTuring dataset. For now we will call the model function one at a time and collect the results in a list. Also, we will collect per-task metrics and the overall metrics for the dataset as we go. Once we're done, we will save the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78868c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.1 Set up data structures for results\n",
    "\n",
    "@dataclass\n",
    "class Result:\n",
    "    id: int\n",
    "    task: str\n",
    "    question: str\n",
    "    answer: str\n",
    "    raw_prediction: Optional[str]\n",
    "    processed_prediction: Optional[str]\n",
    "    score: Optional[float]\n",
    "    success: bool\n",
    "\n",
    "def save_results(results: List[Result], results_csv_filename: str) -> None:\n",
    "    raise NotImplementedError\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56608117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.2 Loop over the dataset with a progress bar\n",
    "\n",
    "# * Do not forget to add the results to our Result list, both successful and failed predictions\n",
    "# * API calls will not always work, so make sure we capture the exceptions from failed calls\n",
    "#    and add them to the Result list with a `status=False`\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "results: List[Result] = []\n",
    "task_scores: Dict[str, List[float]] = defaultdict(list)\n",
    "# Create progress bar\n",
    "for i, row in tqdm(df.iterrows(), total=len(df), desc=\"Evaluating\"):\n",
    "    try:\n",
    "        # Get prediction from model\n",
    "        raw_prediction = query_gene_gpt_model(\n",
    "            client=OpenAI,\n",
    "            system_prompt=system_content,  # Using the system content we defined earlier\n",
    "            example_messages=example_messages,\n",
    "            user_query=row['question'],\n",
    "            config=model_config\n",
    "        )\n",
    "        \n",
    "        # Process the prediction using get_answer function\n",
    "        processed_prediction = get_answer(raw_prediction, row['task'])\n",
    "        \n",
    "        # Calculate score using score_answer function\n",
    "        score = score_answer(processed_prediction, row['answer'], row['task'])\n",
    "\n",
    "        \n",
    "        # Create Result object for successful prediction\n",
    "        result = Result(\n",
    "            id=i,\n",
    "            task=row['task'],\n",
    "            question=row['question'],\n",
    "            answer=row['answer'],\n",
    "            raw_prediction=raw_prediction,\n",
    "            processed_prediction=processed_prediction,\n",
    "            score=score,\n",
    "            success=True\n",
    "        )\n",
    "        task_scores[row['task']].append(score)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i}: {str(e)}\")\n",
    "        # Create Result object for failed prediction\n",
    "        result = Result(\n",
    "            id=i,\n",
    "            task=row['task'],\n",
    "            question=row['question'],\n",
    "            answer=row['answer'],\n",
    "            raw_prediction=None,\n",
    "            processed_prediction=None,\n",
    "            score=None,\n",
    "            success=False\n",
    "        )\n",
    "    \n",
    "    # Add result to our list\n",
    "    results.append(result)\n",
    "    # Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame([vars(r) for r in results])\n",
    "\n",
    "# Print summary statistics\n",
    "print(f\"\\nProcessed {len(results)} examples\")\n",
    "print(f\"Successful predictions: {sum(r.success for r in results)}\")\n",
    "print(f\"Failed predictions: {sum(not r.success for r in results)}\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e2cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dcf5de",
   "metadata": {},
   "source": [
    "Now, let's implement the evaluation loop for the GeneTuring dataset. For now we will call the model function one at a time and collect the results in a list. Also, we will collect per-task metrics and the overall metrics for the dataset as we go. Once we're done, we will save the results to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc013b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6.3 Save the results\n",
    "\n",
    "# 每个任务的平均得分\n",
    "task_avg = {\n",
    "    task: sum(scores) / len(scores) if scores else 0.0\n",
    "    for task, scores in task_scores.items()\n",
    "}\n",
    "\n",
    "# 总体平均得分\n",
    "all_scores = [s for scores in task_scores.values() for s in scores]\n",
    "overall_avg = sum(all_scores) / len(all_scores) if all_scores else 0.0\n",
    "\n",
    "# 打印统计\n",
    "print(\"\\n=== Evaluation Summary ===\")\n",
    "for task, avg in task_avg.items():\n",
    "    print(f\"{task}: {avg:.3f}\")\n",
    "print(f\"\\nOverall average score: {overall_avg:.3f}\")\n",
    "print(f\"Total examples: {len(results)}\")\n",
    "print(f\"Successful: {sum(r.success for r in results)}\")\n",
    "print(f\"Failed: {sum(not r.success for r in results)}\")\n",
    "\n",
    "# 保存结果\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "results_df.to_csv(\"outputs/gene_turing_results.csv\", index=False)\n",
    "print(\"\\nResults saved to outputs/gene_turing_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dab8dd",
   "metadata": {},
   "source": [
    "## 7. Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4efaa",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Now that we have collected the first round of GeneTuring results, let's analyze them. Let's start by calculating what fraction of predictions were successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dcca4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.1 Calculate the fraction of successful predictions\n",
    "total_predictions = len(results_df)\n",
    "successful_predictions = results_df['success'].sum()\n",
    "success_fraction = successful_predictions / total_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46a3607",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Now let's calculate both the overall score as well as the score by task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3f7c69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.2 Calculate the overall score and the score by task\n",
    "success_df = results_df[results_df['success'] == True]\n",
    "overall_score = success_df['score'].mean()\n",
    "overall_score_by_task = success_df.groupby('task')['score'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fc369e",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "Then, let's create a bar chart of the scores by task with a horizontal line for the overall score. Let's save the figure as well to our output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222356e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7.3 Create a bar chart of the scores by task with a horizontal line for the overall score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "print(overall_score_by_task)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(overall_score_by_task.keys(), overall_score_by_task.values)\n",
    "plt.axhline(y=overall_score, color='red', linestyle='--', label=f'Overall Avg: {overall_score:.2f}')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('GeneTuring Task Performance')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# 保存图像\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "output_path = 'outputs/gene_turing_scores_by_task.png'\n",
    "plt.savefig(output_path)\n",
    "\n",
    "output_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9760f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import mlflow\n",
    "import os\n",
    "\n",
    "# 确保输出目录存在\n",
    "os.makedirs(\"outputs\", exist_ok=True)\n",
    "# 1. 把 MLflow 服务器加到 NO_PROXY\n",
    "os.environ[\"NO_PROXY\"] = os.environ.get(\"NO_PROXY\", \"\") + \",198.215.61.34\"\n",
    "os.environ[\"no_proxy\"] = os.environ[\"NO_PROXY\"]               # 有些系统区分大小写\n",
    "\n",
    "# 2. 彻底屏蔽代理变量（对当前 Python 进程）\n",
    "for var in (\"http_proxy\", \"https_proxy\", \"HTTP_PROXY\", \"HTTPS_PROXY\"):\n",
    "    os.environ.pop(var, None)\n",
    "# 设置远程 tracking server\n",
    "mlflow.set_tracking_uri(\"http://198.215.61.34:8153/\")\n",
    "mlflow.set_experiment(\"Yi\")\n",
    "\n",
    "try:\n",
    "    with mlflow.start_run(run_name=\"v1-openai\"):\n",
    "        # 参数记录\n",
    "        mlflow.log_param(\"model_name\", model_config.model_name)\n",
    "        mlflow.log_param(\"backend\", model_config.model_backend)\n",
    "\n",
    "        # 分数记录\n",
    "        mlflow.log_metric(\"overall_score\", overall_avg)\n",
    "        for task, avg in task_avg.items():\n",
    "            mlflow.log_metric(f\"{task.replace(' ', '_')}_score\", avg)\n",
    "\n",
    "        # 上传文件产物\n",
    "        mlflow.log_artifact(\"outputs/gene_turing_scores_by_task.png\", artifact_path=\"figures\")\n",
    "        mlflow.log_artifact(\"outputs/gene_turing_results.csv\", artifact_path=\"predictions\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"[MLflow Error] Logging failed: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
